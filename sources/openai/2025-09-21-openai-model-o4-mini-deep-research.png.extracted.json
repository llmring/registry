[
  {
    "supports_caching": true,
    "model_name": "gpt-4.1",
    "supports_long_context_pricing": false,
    "supports_json_mode": true,
    "dollars_per_million_tokens_input": 2,
    "cache_storage_cost_per_million_tokens_per_hour": 0.5,
    "supports_function_calling": true,
    "supports_streaming": true,
    "description": "GPT-4.1 excels at instruction following and tool calling, with broad knowledge across domains. It features a 1M token context window, and low latency without a reasoning step.",
    "supports_documents": false,
    "is_active": true,
    "model_aliases": [],
    "supports_thinking": false,
    "supports_parallel_tool_calls": true,
    "supports_vision": true,
    "max_output_tokens": 32768,
    "dollars_per_million_tokens_cached_input": 0.5,
    "dollars_per_million_tokens_output": 8,
    "display_name": "GPT-4.1",
    "max_input_tokens": 1014808
  },
  {
    "is_active": true,
    "description": "GPT-5 Nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks.",
    "model_name": "gpt-5-nano",
    "model_aliases": [],
    "dollars_per_million_tokens_input": 0.05,
    "supports_streaming": true,
    "supports_vision": true,
    "dollars_per_million_tokens_output": 0.4,
    "supports_thinking": true,
    "dollars_per_million_tokens_cached_input": 0.005,
    "supports_json_mode": true,
    "max_input_tokens": 272000,
    "supports_function_calling": true,
    "display_name": "GPT-5 nano",
    "supports_caching": true,
    "max_output_tokens": 128000
  },
  {
    "supports_function_calling": false,
    "display_name": "o3-deep-research",
    "dollars_per_million_tokens_cached_input": 2.5,
    "max_output_tokens": 100000,
    "supports_documents": true,
    "description": "o3-deep-research is our most advanced model for deep research, designed to tackle complex, multi-step research tasks. It can search and synthesize information from across the internet as well as from your own data\u2014brought in through MCP connectors.",
    "supports_streaming": true,
    "supports_json_mode": false,
    "dollars_per_million_tokens_output": 40,
    "model_name": "o3-deep-research",
    "is_active": true,
    "max_input_tokens": 100000,
    "supports_thinking": true,
    "supports_vision": true,
    "dollars_per_million_tokens_input": 10,
    "model_aliases": [],
    "supports_caching": true
  },
  {
    "model_name": "o4-mini-deep-research",
    "dollars_per_million_tokens_output": 8,
    "supports_json_mode": false,
    "description": "o4-mini-deep-research is our faster, more affordable deep research model\u2014ideal for tackling complex, multi-step research tasks. It can search and synthesize information from across the internet as well as from your own data, brought in through MCP connectors.",
    "display_name": "o4-mini-deep-research",
    "supports_streaming": true,
    "supports_documents": true,
    "dollars_per_million_tokens_cached_input": 0.5,
    "supports_function_calling": false,
    "max_output_tokens": 100000,
    "dollars_per_million_tokens_input": 2,
    "supports_caching": true,
    "max_input_tokens": 100000,
    "supports_vision": true,
    "model_aliases": [],
    "is_active": true,
    "supports_thinking": true
  }
]