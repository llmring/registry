[
  {
    "is_active": true,
    "max_input_tokens": 272000,
    "description": "GPT-5-Codex is a version of GPT-5 optimized for agentic coding tasks in Codex or similar environments. It's available in the Responses API only and the underlying model snapshot will be regularly updated.",
    "supports_json_mode": true,
    "dollars_per_million_tokens_output": 10,
    "supports_function_calling": true,
    "max_output_tokens": 128000,
    "dollars_per_million_tokens_input": 1.25,
    "model_name": "gpt-5-codex",
    "supports_vision": true,
    "model_aliases": [],
    "supports_caching": true,
    "dollars_per_million_tokens_cached_input": 0.125,
    "supports_streaming": true,
    "display_name": "GPT-5-Codex"
  },
  {
    "dollars_per_million_tokens_input": 1.25,
    "supports_caching": true,
    "supports_json_mode": true,
    "supports_streaming": true,
    "dollars_per_million_tokens_output": 10,
    "is_active": true,
    "supports_function_calling": true,
    "display_name": "GPT-5",
    "description": "GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains.",
    "model_name": "gpt-5-2025-08-07",
    "model_aliases": [
      "gpt-5-codex",
      "gpt-5"
    ],
    "dollars_per_million_tokens_cached_input": 0.125,
    "max_output_tokens": 128000,
    "supports_thinking": true,
    "supports_vision": true,
    "max_input_tokens": 272000
  },
  {
    "supports_caching": true,
    "description": "GPT-5 mini is a faster, more cost-efficient version of GPT-5. It's great for well-defined tasks and precise prompts.",
    "supports_streaming": true,
    "supports_function_calling": true,
    "dollars_per_million_tokens_input": 0.25,
    "is_active": true,
    "model_name": "gpt-5-mini-2025-08-07",
    "max_output_tokens": 128000,
    "dollars_per_million_tokens_cached_input": 0.025,
    "model_aliases": [
      "gpt-5-mini"
    ],
    "display_name": "GPT-5 mini",
    "dollars_per_million_tokens_output": 2,
    "max_input_tokens": 272000,
    "supports_json_mode": true,
    "supports_vision": true,
    "supports_thinking": true
  },
  {
    "max_output_tokens": 128000,
    "model_name": "gpt-5-nano-2025-08-07",
    "model_aliases": [
      "gpt-5-nano"
    ],
    "supports_streaming": true,
    "supports_thinking": true,
    "supports_caching": true,
    "max_input_tokens": 272000,
    "supports_function_calling": true,
    "supports_vision": true,
    "dollars_per_million_tokens_output": 0.4,
    "description": "GPT-5 Nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks.",
    "dollars_per_million_tokens_cached_input": 0.005,
    "dollars_per_million_tokens_input": 0.05,
    "is_active": true,
    "supports_json_mode": true,
    "display_name": "GPT-5 nano"
  },
  {
    "dollars_per_million_tokens_output": 120,
    "supports_documents": true,
    "supports_vision": true,
    "description": "GPT-5 pro uses more compute to think harder and provide consistently better answers. GPT-5 pro is available in the Responses API only to enable support for multi-turn model interactions before responding to API requests, and other advanced API features in the future. Since GPT-5 pro is designed to tackle tough problems, some requests may take several minutes to finish. To avoid timeouts, try using background mode. As our most advanced reasoning model, GPT-5 pro defaults to (and only supports) reasoning.effort: high. GPT-5 pro does not support code interpreter.",
    "model_name": "gpt-5-pro-2025-10-06",
    "max_input_tokens": 128000,
    "supports_function_calling": true,
    "display_name": "GPT-5 pro",
    "model_aliases": [
      "gpt-5-pro"
    ],
    "supports_streaming": false,
    "supports_json_mode": true,
    "is_active": true,
    "dollars_per_million_tokens_input": 15,
    "max_output_tokens": 272000,
    "supports_thinking": true
  },
  {
    "supports_function_calling": false,
    "supports_json_mode": false,
    "display_name": "o3-deep-research",
    "supports_streaming": true,
    "dollars_per_million_tokens_cached_input": 2.5,
    "description": "o3-deep-research is our most advanced model for deep research, designed to tackle complex, multi-step research tasks. It can search and synthesize information from across the internet as well as from your own data\u2014brought in through MCP connectors.",
    "supports_vision": true,
    "dollars_per_million_tokens_output": 40,
    "max_input_tokens": 100000,
    "supports_caching": true,
    "model_name": "o3-deep-research-2025-06-26",
    "max_output_tokens": 100000,
    "is_active": true,
    "supports_thinking": true,
    "dollars_per_million_tokens_input": 10,
    "model_aliases": [
      "o3-deep-research"
    ]
  },
  {
    "supports_json_mode": false,
    "model_aliases": [
      "o4-mini-deep-research"
    ],
    "supports_streaming": true,
    "display_name": "o4-mini-deep-research",
    "max_output_tokens": 100000,
    "is_active": true,
    "dollars_per_million_tokens_output": 8,
    "supports_thinking": true,
    "description": "o4-mini-deep-research is our faster, more affordable deep research model\u2014ideal for tackling complex, multi-step research tasks. It can search and synthesize information from across the internet as well as from your own data, brought in through MCP connectors.",
    "max_input_tokens": 100000,
    "dollars_per_million_tokens_input": 2,
    "model_name": "o4-mini-deep-research-2025-06-26",
    "supports_vision": true,
    "dollars_per_million_tokens_cached_input": 0.5,
    "supports_caching": true,
    "supports_function_calling": false
  }
]